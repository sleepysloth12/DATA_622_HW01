---
title: "Data 622 Homework 4"
author: "Brandon Cunningham, Jean Jimenez, Chafiaa Nadour, Shri Tripathi"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document:
    df_print: paged
---

# Introduction: For this assignment we will be using a kaggle dataset on houses to predict house prices using features of the house. [Link to Dataset](https://www.kaggle.com/datasets/zafarali27/house-price-prediction-dataset?resource=download). For prediction we will be using a random forest model and a nueral network, giving both the same data to work with and seeing which is better at this specific task.

```{r}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggcorrplot)
library(GGally)
library(VIM)
library(gridExtra)
library(caret)
library(leaps)
library(boot)
library(summarytools)
library(corrplot)
library(fastDummies)
library(car)
library(rpart)
library(rpart.plot)
library(dplyr)
```

Our first step is to load in the data and check for missing values.

```{r}
data <- read.csv("C:/Users/Chafiaa/Downloads/House Price Prediction Dataset.csv")
any(is.na(data))
```

## Since there are no missing values we can skip the imputation step and go straight to transformations. Firstly we want to convert the column condition to numeric. Since it is an ordinal column where there is a ranking to the conditions we convert them to 0-3 instead of using dummies.
```{r}
# Convert Condition to numeric
data$Condition <- as.numeric(factor(data$Condition, levels = c("Poor", "Fair", "Good", "Excellent"))) - 1
```
## Next we convert Garage from a Yes/No categorical column to a 0/1 column.
```{r}
data$Garage <- factor(ifelse(data$Garage == "Yes", 1, 0))
```
##  Lastly we want to one hot encode the location column as it is not ordinal and view the new dataset.
```{r}
data <- dummy_cols(data, select_columns = "Location", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
head(data)
```


```{r}
summary(data)
```


```{r}

data1=data %>% select(where(is.numeric))
head(data1)

```


```{r}
correlations <- cor(na.omit(data1[,-1]))

corr <- apply(correlations, 1, function(x) sum(x > 0.3 | x < -0.3) > 1)
correlations<- correlations[corr ,corr ]
corrplot(correlations, method="square")
```





##  RF model:
```{r}
library(randomForest)

# Splitting the dataset into training and testing:

set.seed(123)
training <- sample(1:nrow(data), 0.8 * nrow(data))
train_data <- data1[training, ]
test_data <- data1[-training, ]

# Train the Random Forest model
rf_model <- randomForest(Price ~ ., data = train_data, ntree = 500, mtry = 3)
# Evaluate the model
predicted_prices <- predict(rf_model, test_data)
```





```{r}


# importance of each variable.


library(ggplot2)
var_importance <- data_frame(variable=setdiff(colnames(train_data), "Price"),
                             importance=as.vector(importance(rf_model)))
var_importance <- arrange(var_importance, desc(importance))
var_importance$variable <- factor(var_importance$variable, levels=var_importance$variable)

p <- ggplot(var_importance, aes(x=variable, weight=importance, fill=variable))
p <- p + geom_bar() + ggtitle("Variable Importance from Random Forest Fit")
p <- p + xlab("Demographic Attribute") + ylab("Variable Importance (Mean Decrease in Gini Index)")
p <- p + scale_fill_discrete(name="Variable Name")
p + theme(axis.text.x=element_blank(),
          axis.text.y=element_text(size=12),
          axis.title=element_text(size=16),
          plot.title=element_text(size=18),
          legend.title=element_text(size=16),
          legend.text=element_text(size=12))

```


```{r}

ggplot(data, ylab = "Sale Price  (USD)", aes(x=Price)) + 
    geom_histogram(bins=30, fill="red") +
    ggtitle("Figure 3: Distribution of Sale Prices After randomForest Imputation") 

```


```{r}
print(rf_model)


```


```{r}

# predicted_prices are the randomForest predictions and test_data contains the real prices
actual_prices <- test_data$Price

# Calculate mean squared error 
mse <- mean((predicted_prices - actual_prices)^2)

# Calculate root mean squared error
rmse <- sqrt(mse)

# Calculate mean absolute error
mae <- mean(abs(predicted_prices - actual_prices))

# Calculate r-squared
rss <- sum((predicted_prices - actual_prices)^2)
tss <- sum((actual_prices - mean(actual_prices))^2)
r_squared <- 1 - rss/tss

# Print the metrics
print(paste("MSE:", mse))

```


```{r}
print(paste("RMSE:", rmse))
```


```{r}
print(paste("MAE:", mae))
```


```{r}
print(paste("R-squared:", r_squared))
```
# The Random Forest model produced very week results, with a high MSE on the housing price dataset even though we used only the numerical columns, it probably the data is from known surce and collected carfully also it very small data set.


##  Neural Network Model:
```{r}


set.seed(123)
nnet_model <- train(Price ~., 
                    data = train_data, 
                    method = "nnet", 
                    trControl = trainControl(method = "cv", number = 10),
                    trace=0)

nnet_model





```


```{r}

print(nnet_model)

```
# The Neutral network model produced very week results, with a high MAE on the housing price dataset even though we used only the numerical columns, it probably the data is not image data



