---
title: "Data 622 Homework 4"
author: "Brandon Cunningham, Jean Jimenez, Chafiaa Nadour, Shri Tripathi"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document:
    df_print: paged
---

# Introduction: For this assignment we will be using a kaggle dataset on houses to predict house prices using features of the house. [Link to Dataset](https://www.kaggle.com/datasets/zafarali27/house-price-prediction-dataset?resource=download). For prediction we will be using a random forest model and a nueral network, giving both the same data to work with and seeing which is better at this specific task.

```{r}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggcorrplot)
library(GGally)
library(VIM)
library(gridExtra)
library(caret)
library(leaps)
library(boot)
library(summarytools)
library(corrplot)
library(fastDummies)
library(car)
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
```

Our first step is to load in the data and check for missing values.
```{r}
data <- read.csv(url("https://raw.githubusercontent.com/sleepysloth12/DATA_622_HW01/refs/heads/main/homework4/House%20Price%20Prediction%20Dataset.csv"))
any(is.na(data))
```
Next we want to remove the ID column as that is unnessecary for prediction
```{r}
data <- data %>% 
  select(-Id) 
```
Since there are no missing values we can skip the imputation step and go straight to transformations. Firstly we want to convert the column condition to numeric. Since it is an ordinal column where there is a ranking to the conditions we convert them to 0-3 instead of using dummies.
```{r}
# Convert Condition to numeric
data$Condition <- as.numeric(factor(data$Condition, levels = c("Poor", "Fair", "Good", "Excellent"))) - 1
```
Next we convert Garage from a Yes/No categorical column to a 0/1 column.
```{r}
data$Garage <- ifelse(data$Garage == "Yes", 1, 0)
```
Lastly we want to one hot encode the location column as it is not ordinal and view the new dataset.
```{r}
data <- dummy_cols(data, select_columns = "Location", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
head(data)
```
## Correlation Matrix
Before building the models, we want to run a correlation matrix to look for multicolinearity
```{r}
predictors <- data %>% 
  select(-Price) 
cor_matrix <- cor(predictors)
high_cor <- findCorrelation(cor_matrix, cutoff = 0.7, verbose = TRUE)
cat("Highly correlated variables:", paste(names(predictors)[high_cor], collapse = ", "), "\n")
```
Initial findings show no highly correlated variables.
```{r}
corrplot(cor_matrix, method = "color", type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
And the maxtrix confirms little to no correlation between different variables, there is no multicolinearity.
### Principal Component Analysis
Conducting a PCA to determine the important components.
```{r}
pca_result <- prcomp(predictors, scale. = TRUE)
summary(pca_result)
```
```{r}
plot(cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2)), 
     type = "b", 
     xlab = "Number of Components", 
     ylab = "Cumulative Proportion of Variance Explained",
     main = "PCA Cumulative Variance Explained")
```
Our first component explains about 13% which us a good amount, but the more interesting part is that every component until the 9th compnent explains at least 9% with only the 10th component explaining a small by comparison 4%. 
What this suggest is that every column in the dataset is important and that we should not try to simplify the model as there are no components that explain an insignificant amount and all of the varibles contain unique information in them. 



```{r}

data1=data %>% select(where(is.numeric))
head(data1)

```


```{r}
correlations <- cor(na.omit(data1[,-1]))

corr <- apply(correlations, 1, function(x) sum(x > 0.3 | x < -0.3) > 1)
correlations<- correlations[corr ,corr ]
corrplot(correlations, method="square")
```





##  RF model:
```{r}
library(randomForest)

# Splitting the dataset into training and testing:

set.seed(123)
training <- sample(1:nrow(data), 0.8 * nrow(data))
train_data <- data[training, ]
test_data <- data[-training, ]
```


Scaling the data to produce better results
```{r}
preprocess <- preProcess(train_data, method = c("center", "scale"))
train_data_scaled <- predict(preprocess, train_data)
test_data_scaled <- predict(preprocess, test_data)
```

Training the model using 5 fold cross validation and a tuning grid to find the best mtry value.
```{r}
set.seed(123)
tune_grid <- expand.grid(mtry = c(2, 3, 4, 5))
control <- trainControl(method = "cv", number = 5) 

rf_model <- train(
  Price ~ ., 
  data = train_data_scaled, 
  method = "rf", 
  trControl = control, 
  tuneGrid = tune_grid, 
  ntree = 200 
)
```



```{r}
print(rf_model$bestTune)
```

```{r}
var_importance <- varImp(rf_model, scale = FALSE)
print(var_importance)
```
```{r}
predicted_prices_scaled <- predict(rf_model, newdata = test_data_scaled)

predicted_prices <- predicted_prices_scaled * preprocess$std[["Price"]] + preprocess$mean[["Price"]]
```





# Train the Random Forest model
rf_model <- randomForest(Price ~ ., data = train_data, ntree = 500, mtry = 3)
# Evaluate the model
predicted_prices <- predict(rf_model, test_data)






```{r}


# importance of each variable plotted
var_importance_df <- data.frame(
  variable = rownames(var_importance$importance),
  importance = var_importance$importance[, 1]
)

var_importance_df <- arrange(var_importance_df, desc(importance))
var_importance_df$variable <- factor(var_importance_df$variable, levels = var_importance_df$variable)


p <- ggplot(var_importance_df, aes(x = variable, weight = importance, fill = variable)) +
  geom_bar() +
  ggtitle("Variable Importance from Random Forest Fit") +
  xlab("Demographic Attribute") +
  ylab("Variable Importance (Mean Decrease in Gini Index)") +
  scale_fill_discrete(name = "Variable Name") +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_text(size = 12),
    axis.title = element_text(size = 16),
    plot.title = element_text(size = 18),
    legend.title = element_text(size = 16),
    legend.text = element_text(size = 12)
  )

print(p)

```


```{r}

ggplot(data, ylab = "Sale Price  (USD)", aes(x=Price)) + 
    geom_histogram(bins=30, fill="red") +
    ggtitle("Figure 3: Distribution of Sale Prices After randomForest Imputation") 

```


```{r}
print(rf_model)


```


```{r}

# predicted_prices are the randomForest predictions and test_data contains the real prices
actual_prices <- test_data$Price

# Calculate mean squared error 
mse <- mean((predicted_prices - actual_prices)^2)

# Calculate root mean squared error
rmse <- sqrt(mse)

# Calculate mean absolute error
mae <- mean(abs(predicted_prices - actual_prices))

# Calculate r-squared
rss <- sum((predicted_prices - actual_prices)^2)
tss <- sum((actual_prices - mean(actual_prices))^2)
r_squared <- 1 - rss/tss

# Print the metrics
print(paste("MSE:", mse))

```


```{r}
print(paste("RMSE:", rmse))
```


```{r}
print(paste("MAE:", mae))
```


```{r}
print(paste("R-squared:", r_squared))
```
# The Random Forest model produced very week results, with a high MSE on the housing price dataset even though we used only the numerical columns, it probably the data is from known surce and collected carfully also it very small data set.


##  Neural Network Model:
```{r}


set.seed(123)
nnet_model <- train(Price ~., 
                    data = train_data, 
                    method = "nnet", 
                    trControl = trainControl(method = "cv", number = 10),
                    trace=0)

nnet_model





```


```{r}

print(nnet_model)

```
# The Neutral network model produced very week results, with a high MAE on the housing price dataset even though we used only the numerical columns, it probably the data is not image data



