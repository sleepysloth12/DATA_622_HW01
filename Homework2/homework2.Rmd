---
title: "Homework2"
author: "Brandon Cunningham"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggcorrplot)
library(GGally)
library(VIM)
library(gridExtra)
library(caret)
library(leaps)
library(boot)
library(summarytools)
library(corrplot)
library(car)
library(rpart)
library(dplyr)
```

# Large Datset (Diabetes Data)

For our large dataset, we will use the diabetes dataset from kaggle.

This dataset has 100k clinical records of diabetes for health analytic purposes.

[Link to Dataset](https://www.kaggle.com/datasets/priyamchoksi/100000-diabetes-clinical-dataset?resource=download)

Goal: For this dataset, we want to predict whether or not the patient will have diabetes.

Importing:

```{r}
diabetes_data=read.csv(url("https://raw.githubusercontent.com/sleepysloth12/DATA_622_HW01/refs/heads/main/diabetes_dataset.csv"))
```

## Exploratory Data Analysis

The column of interest, labeled `diabetes` is what we want to predict. It is an integer, 0 or 1, indicating if the patient has diabetes or not. In the current dataset, 91% of the patients have no diabetes and 8.5% of the patients have diabetes.

In order to build a predictive model, we must first go column by column and clean up the features a little bit to make this more accurate/applicable to healthcare data.

### Data Cleaning

#### Year

The first column is year. The dataset is timeseries data, collected from the years 2015-2022. However, each year has different numbers of observations. There is no way of knowing if this is longtitudnal data (one patient visited multiple year) due to the lack of unique patient identifier field. I think we can completely disregard and forget about this column.

```{r}
diabetes_data = diabetes_data %>%
  select(-year)
```

#### Gender

Next is gender. Gender is pretty even split, with \~60% being female and \~40% being male. There is an insignificant amount of people that answered "other" (less than 1%).

I'm going ahead and going to filter out other. Also, I am going to change the label to`is_female` so the choice is binary.

```{r}

diabetes_data = diabetes_data %>%
  filter(gender == "Female"| gender== "Male")%>%
  mutate(is_female=ifelse(gender == "Female",1,0))%>%
  select(-gender)
```

#### Age

Next is age. Mean age is 41.9 years old, with a standard deviation of +/-22.5 years old.

Max age is 80.

Minimum recorded age is 0.08. This might be an outlier. Therefore, lets visualize this distribution in both box plot and bar plot.

```{r}
ggplot(diabetes_data, aes(x = age)) +
  geom_histogram(binwidth = 5, color = "black", fill = "lightblue") +
  labs(title = "Distribution of Age", x = "Age", y = "Count") +
  theme_minimal()
```

```{r}
ggplot(diabetes_data, aes(y = age)) +
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(title = "Boxplot of Age", y = "Age") +
  theme_minimal()

```

Seems like the minimum age is an outlier. In medical research, we tend to separate adult populations from pediatric populations so lets go ahead and do that here. Lets only look at 18+.

In terms of the age distribution, it looks relatively normal. Diabetes incidence seem to increase as you get closer to middle age, then decrease. There is a spike at 80 years old.

I am going to bin age/ convert it into different categories:

`is_young` = Age 18-35

`is_middle_age` = Age 36-64

`is_old` = Age 65+

```{r}

diabetes_data = diabetes_data %>%
  filter(age>=18)%>%
  mutate(is_young=ifelse(age>=18 & age <=35, 1,0),
         is_middle_age=ifelse(age>35 & age<65 , 1 , 0),
         is_old=ifelse(age>65,1,0))%>%
  select(-age)

```

#### State

```{r}
length(unique(diabetes_data$location))
```

For the location column, there are 55 different locations, corresponding to the 50 different states and territory.

Location is important for diabetes prediction. Some areas are probably more likely to develop diabetes than others. Like age, I want to create categories and bin them based on the location. Then, will create dummy variables.

```{r}
diabetes_data = diabetes_data %>%
  mutate(
    
    is_new_england = if_else(location %in% c("Connecticut", "Maine", "Massachusetts", 
                                          "New Hampshire", "Rhode Island", "Vermont"), 1, 0),

    
    is_south = if_else(location %in% c("Alabama", "Arkansas", "Delaware", "Florida", "Georgia", 
                                    "Kentucky", "Louisiana", "Maryland", "Mississippi", 
                                    "North Carolina", "Oklahoma", "South Carolina", 
                                    "Tennessee", "Texas", "Virginia", "West Virginia"), 1, 0),

    
    is_midwest = if_else(location %in% c("Illinois", "Indiana", "Iowa", "Kansas", "Michigan", 
                                      "Minnesota", "Missouri", "Nebraska", "North Dakota", 
                                      "Ohio", "South Dakota", "Wisconsin"), 1, 0),

    
    is_west = if_else(location %in% c("Alaska", "Arizona", "California", "Colorado", "Hawaii", 
                                   "Idaho", "Montana", "Nevada", "New Mexico", "Oregon", 
                                   "Utah", "Washington", "Wyoming"), 1, 0),

    
    is_northeast = if_else(location %in% c("New Jersey", "New York", "Pennsylvania"), 1, 0),

    
    is_territories = if_else(location %in% c("Guam", "Puerto Rico", "Virgin Islands", 
                                          "District of Columbia", "United States"), 1, 0)
  )%>%
  select(-location)
```

#### Race, Ethnicity, Hypertension, & Heart Disease

Race and ethnicity is already binned and with their individual dummy variables. Race and ethnicity are both factors that influence diabetes so will leave these columns untouched.

Same with the columns of hypertension and heart disease.

#### Smoking History

There are currently 6 categories/ choices patients could respond when asked about smoking history:

```{r}
unique(diabetes_data$smoking_history)
```

```{r}
ggplot(diabetes_data, aes(x = smoking_history)) +
  geom_bar(fill = "lightblue", color = "black") +
  labs(title = "Distribution of Smoking History", x = "Smoking History", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

The biggest group is Never smoked accounting for 35% of the data.

There is a category, 'ever' which is 'Never' mislabeled. Will fix this. Once combined, never smoked will account for40% of the data.

The second biggest is 'No info' with near 35% of the data. Since the people in 'No info' may or may not be smokers, if we leave this category in it might make our predictions inaccurate. We want to capture how smoking can influence diabetes, therefore we ill remove this group.

Also, the 'not current' and 'former' group can be combined.

```{r}
diabetes_data = diabetes_data %>%
  filter(smoking_history!="No Info")%>%
  mutate(never_smoked=ifelse(smoking_history %in% c("ever","never"),1,0),
         former_smoker=ifelse(smoking_history %in% c("former","not current"),1,0),
         current_smoker=ifelse(smoking_history=="current",1,0)) %>%
  select(-smoking_history)
```

#### Biomarker Columns

The distribution of BMI is normal. It is numeric and continuous. We are leaving this as is.

The hbA1c_level biomarker, although numeric, has 18 unique values. In healthcare, this biomarker is usually used to determine diabetes. We will bin this biomarker for the following categories:

A1c \< 5.7% –\> Normal A1C

A1c between 5.7-6.4 % –\> PreDiabetes

A1C over 6.5% –\> diabetes

Although, correlation analysis is needed. There might be multicollinearity between these biomarker variables.

I say this because blood glucose variable and A1c directly related to each other.

Actually going to remove blood glucose because having that and A1C is repetitive/ multicollinearity.

```{r}
diabetes_data = diabetes_data %>%
  mutate(normal_a1c=ifelse(hbA1c_level<5.7,1,0),
         prediabetic_a1c=ifelse(hbA1c_level>=5.7 & hbA1c_level <= 6.4,1,0),
         diabetic_a1c=ifelse(hbA1c_level>6.4,1,0))%>%
  select(-c(hbA1c_level,blood_glucose_level))
```

## Model Selection

Now that our dataset is clean, we can discuss what model we want to use.

The target variable to predict is diabetes (binary choice whether or not patient will have diabetes).

I think the best algorithm to use in this case is logistic regression. Logistic regression provides interpretable results. The coefficients in the model can be easily interpreted as the change in log-odds of having diabetes for a one-unit change in the predictor, holding other variables constant. This interpretability is important in healthcare.

### Correlation Matrix

Before begining the logistic regression model, I want to run a correlation matrix to look for multicolinearity

```{r}
predictors <- diabetes_data %>% 
  select(-diabetes) 

cor_matrix <- cor(predictors)

high_cor <- findCorrelation(cor_matrix, cutoff = 0.7, verbose = TRUE)

cat("Highly correlated variables:", paste(names(predictors)[high_cor], collapse = ", "), "\n")
```

```{r}
corrplot(cor_matrix, method = "color", type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

There is some multicollinearity

### Principal Component Analysis

Conducting a PCA to determine the important components

```{r}
pca_result <- prcomp(predictors, scale. = TRUE)
summary(pca_result)
```

```{r}
plot(cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2)), 
     type = "b", 
     xlab = "Number of Components", 
     ylab = "Cumulative Proportion of Variance Explained",
     main = "PCA Cumulative Variance Explained")
```

Our first principal component only accounts for about 8.16% of the total variance. That's not a lot. It means no single factor dominates in predicting diabetes. This makes sense given the complex nature of the disease and the variety of factors we've included in our dataset.

We need 11 components to explain about 66% of the variance, and it takes 19 to get to nearly 100%. Looking at our cumulative variance plot, we can see this gradual climb. The fact that we need so many components to explain most of the variance suggests we shouldn't try to oversimplify our model. Most of our variables are contributing unique information about diabetes risk.

While we don't see extreme multicollinearity, there is some correlation among our variables. We can explain about 85% of the variance with 15 components, which is fewer than our original variables.


```{r}

set.seed(622)

train_split_idx=createDataPartition(diabetes_data$diabetes, p=0.7, list=FALSE)

train_diab = diabetes_data[train_split_idx,]
test_diab = diabetes_data[-train_split_idx,]

control <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"


```

Decision Tree 1:

For this tree we are throwing everything into the decision tree and 

```{r}

set.seed(622)
fit_tree <- rpart(diabetes ~ ., method = 'class', data = train_diab)

plotcp(fit_tree)


```
```{r}
printcp(fit_tree)
```
```{r}
summary(fit_tree)
```
```{r}
par(mfrow = c(1, 2)) 
rsq.rpart(fit_tree)

predictions_tree <- predict(fit_tree, newdata = test_diab)
head(predictions_tree)

accuracy <- mean(predictions_tree == test_diab$diabetes)
print(accuracy)
```
```{r}
test_diab$diabetes <- as.factor(test_diab$diabetes) 
predictions_tree <- predict(fit_tree, newdata = test_diab, type = "class")
conf_matrix <- confusionMatrix(predictions_tree, test_diab$diabetes)

print(conf_matrix$table)
```




